{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9708d9dea6e34c0faa6011fe686fa818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pdfplumber\n",
    "import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(name, load_in_4bit=True, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pdf and extract the name\n",
    "doc_path = \"pdf_folder/The_advantage_of_short_paper_titles.pdf\"\n",
    "doc_name = doc_path.split(\"/\")[-1].split(\".\")[0]\n",
    "doc = \"\"\n",
    "\n",
    "with pdfplumber.open(doc_path) as pdf:\n",
    "\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        text = text.replace(\"Safety Statement\", \"\")\n",
    "        doc += text\n",
    "\n",
    "samp_text = doc\n",
    "word_split = samp_text.split()\n",
    "chunk = \"\"\n",
    "\n",
    "\"\"\" \n",
    "This section breaks down the text into chunks that can be taken by an LLM (context_window).\n",
    "You can include an overlap between chunks, to make sure the LLM has the entire context before \n",
    "making a prediction and fixing the text\n",
    "\"\"\"\n",
    "\n",
    "chunk_size = 100\n",
    "overlap = 0\n",
    "n_chunks = ((len(word_split) - chunk_size) // (chunk_size - overlap)) + 1\n",
    "lstd_text = []\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start = i * (chunk_size - overlap)\n",
    "    end = start + chunk_size if i < n_chunks - 1 else len(word_split)\n",
    "    chunk = \" \".join(word_split[start:end])\n",
    "    lstd_text.append(chunk)\n",
    "    chunk = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 13/13 [08:49<00:00, 40.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: Gonçalves, Perra, Vespignani. 2011 Modeling allometric scaling and the decreasing need for new users’ activity on Twitter networks: validation of words. Science Rep. 2, 943. (doi: 10.1038/srep00943) 35. Yogatama, Heilman, O’Connor, Dyer, Dunbar’s number. PLOS ONE 6, e22656. (doi: 10.1371/journal.pone.0022656) 36. Sakaki, Okazaki, Matsuo. 2010 Earthquake routing Twitter users: real-time event detection by community’s response to an article. In Proc. EMNLP, 9. (doi: 10.1145/22.0001) 37. Mocanu, Baronchelli, Perra, Gonçalves. 2010 Social sensors in Twitter networks. In WWW’10, 26–30 April 2010, 27–31 July 2011, Edinburgh, UK., pp. 594–604. (doi: 10.1109/WWW.2010.555895) 38. Zhang, Vespignani. 2013 The Twitter of Babel: Raleigh, NC., pp. 851–860. New York, NY: ACM. Stroudsburg, PA: Association for Computational Linguistics. 23. Preis, Moat, Bishop, Tre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fxd_text = []\n",
    "text_cue = \"Corrected text:\"\n",
    "tplt_prompt = f\"\"\"I need you to fix the grammatical errors and properly separate words that are stitched together within a text. Only reply with \"Corrected text:\" \"\"\"\n",
    "\n",
    "for item in tqdm.tqdm(lstd_text, desc=\"Processing text\"):\n",
    "    \n",
    "    prompt = f\"\"\"{tplt_prompt}. Original text: \"{item}\". Corrected text:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=chunk_size*4)\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    first_time_idx = output_text.find(text_cue)\n",
    "    sec_time_idx = output_text.find(text_cue, first_time_idx + len(text_cue))\n",
    "    \n",
    "    clear_output(wait=True)     # Clear previous output\n",
    "    corrected_text = output_text[sec_time_idx + len(text_cue):].strip()\n",
    "    print(\"Corrected Text:\", corrected_text)  # Debug print\n",
    "    fxd_text.append(corrected_text)\n",
    "    \n",
    "with open(f\"reviewed_pdfs/{doc_name}.csv\", \"w\") as f:\n",
    "    for item in fxd_text:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
